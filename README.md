# A Brief Overview of Multimodal Large Language Models

This repository contains the compiled PDF for my survey paper on multimodal large language models (MLLMs). The work was completed as part of a university course project and is intended for research, learning, and portfolio purposes. 

## Overview

The paper provides a concise overview of recent developments in multimodal LLMs. Because this was a 5-page course assignment, the discussion is intentionally high-level and focuses on the most essential concepts, covering:
- Core architectural components
- Training strategies (pre-training, alignment, instruction tuning)
- Evaluation challenges and benchmarks
- Applications
- Current limitations and open research directions

The focus is on clarity, compactness, and synthesizing recent work in the field. The survey is designed to serve as an accessible reference for students and researchers interested in modern multimodal systems. 

## Citation

If you want to reference this work, you may cite it informally as:

```
Özlem Karabulut. “A Brief Overview of Multimodal Large Language Models.”  
Course Project, Department of Computational Linguistics, University of Tübingen, 2025.
```

## License

This is a non-official academic project intended strictly for educational and demonstrational use.
Feel free to read, learn from, and adapt the material with appropriate attribution.
